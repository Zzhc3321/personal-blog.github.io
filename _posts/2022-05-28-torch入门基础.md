---
layout: post
title: 2022-05-28-torch入门基础 
tags: [深度学习,机器学习]
category: 深度学习
toc: true
math: true
author: zzhc
---


## 数据操作


```python
import torch
```

&emsp;&emsp;张量表示由一个数值组成的数组，这个数组可能有多个维度。 具有一个轴的张量对应数学上的向量（vector）； 具有两个轴的张量对应数学上的矩阵（matrix）； 具有两个轴以上的张量没有特殊的数学名称。

### 数据基础


```python
x = torch.arange(12)
x
```




    tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])




```python
x.shape
```




    torch.Size([12])




```python
#要想改变一个张量的形状而不改变元素数量和元素值，可以调用reshape函数。
X = x.reshape(3, 4)
X
```




    tensor([[ 0,  1,  2,  3],
            [ 4,  5,  6,  7],
            [ 8,  9, 10, 11]])




```python
#如果我们的目标形状是（高度,宽度），那么在知道宽度后，高度会被自动计算得出，不必我们自己做除法。
x.reshape(-1,4)
```




    tensor([[ 0,  1,  2,  3],
            [ 4,  5,  6,  7],
            [ 8,  9, 10, 11]])




```python
torch.zeros((2, 3, 4))
torch.ones((2, 3, 4))

#正太分布（0，1）正态分布
torch.randn(3, 4)
```




    tensor([[-0.0805, -0.9043, -1.0997, -0.3209],
            [ 1.0052,  0.9480,  0.0824, -2.4924],
            [ 1.1952,  0.8475, -0.0880, -0.1549]])




```python
torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])
```




    tensor([[2, 1, 4, 3],
            [1, 2, 3, 4],
            [4, 3, 2, 1]])



### 运算符


```python
#对于任意具有相同形状的张量， (+、-、*、/和**）都按元素运算。
x = torch.tensor([1.0, 2, 4, 8])
y = torch.tensor([2, 2, 2, 2])
x + y
```




    tensor([ 3.,  4.,  6., 10.])




```python
torch.exp(x)
```




    tensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03])




```python
#我们也可以把多个张量连结（concatenate）在一起
X = torch.arange(12, dtype=torch.float32).reshape((3,4))
Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])
torch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1)
```




    (tensor([[ 0.,  1.,  2.,  3.],
             [ 4.,  5.,  6.,  7.],
             [ 8.,  9., 10., 11.],
             [ 2.,  1.,  4.,  3.],
             [ 1.,  2.,  3.,  4.],
             [ 4.,  3.,  2.,  1.]]),
     tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],
             [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],
             [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]]))




```python
X == Y
```




    tensor([[False,  True, False,  True],
            [False, False, False, False],
            [False, False, False, False]])



### 广播机制

&emsp;&emsp;在某些情况下，即使形状不同，我们仍然可以通过调用 广播机制（broadcasting mechanism）来执行按元素操作。


```python
a = torch.arange(3).reshape((3, 1))
b = torch.arange(2).reshape((1, 2))
a, b
```




    (tensor([[0],
             [1],
             [2]]),
     tensor([[0, 1]]))




```python
#矩阵a将复制列， 矩阵b将复制行，然后再按元素相加。
a + b
```




    tensor([[0, 1],
            [1, 2],
            [2, 3]])



### 索引与切片


```python
X[-1], X[1:3]
```




    (tensor([ 8.,  9., 10., 11.]),
     tensor([[ 4.,  5.,  6.,  7.],
             [ 8.,  9., 10., 11.]]))




```python
X[1, 2] = 9
X
```




    tensor([[ 0.,  1.,  2.,  3.],
            [ 4.,  5.,  9.,  7.],
            [ 8.,  9., 10., 11.]])




```python
X[0:2, :] = 12
X
```




    tensor([[12., 12., 12., 12.],
            [12., 12., 12., 12.],
            [ 8.,  9., 10., 11.]])



### 节省内存

&emsp;&emsp;运行一些操作可能会导致为新结果分配内存。 例如，如果我们用Y = X + Y，我们将取消引用Y指向的张量，而是指向新分配的内存处的张量。


```python
before = id(Y)
Y = Y + X
id(Y) == before
```




    False



&emsp;&emsp;首先，我们不想总是不必要地分配内存。 在机器学习中，我们可能有数百兆的参数，并且在一秒内多次更新所有参数。 通常情况下，我们希望原地执行这些更新。 其次，如果我们不原地更新，其他引用仍然会指向旧的内存位置， 这样我们的某些代码可能会无意中引用旧的参数。


```python
Z = torch.zeros_like(Y)
print('id(Z):', id(Z))
Z[:] = X + Y
print('id(Z):', id(Z))
```

    id(Z): 1810851012480
    id(Z): 1810851012480
    

&emsp;&emsp;我们也可以使用X[:] = X + Y或X += Y来减少操作的内存开销。


```python
before = id(X)
X += Y
id(X) == before
```




    True



### 转换为其他Python对象


```python
A = X.numpy()
B = torch.tensor(A)
type(A), type(B)
```




    (numpy.ndarray, torch.Tensor)




```python
a = torch.tensor([3.5])
a, a.item(), float(a), int(a)
```




    (tensor([3.5000]), 3.5, 3.5, 3)



<br>
<br>

***

## 数据预处理


```python
import os

data_file = os.path.join('data', 'torch_learning_data', 'house_tiny.csv')
with open(data_file, 'w') as f:
    f.write('NumRooms,Alley,Price\n')  # 列名
    f.write('NA,Pave,127500\n')  # 每行表示一个数据样本
    f.write('2,NA,106000\n')
    f.write('4,NA,178100\n')
    f.write('NA,NA,140000\n')
```

### 读取数据集


```python
import pandas as pd

data = pd.read_csv(data_file)
print(data)
```

       NumRooms Alley   Price
    0       NaN  Pave  127500
    1       2.0   NaN  106000
    2       4.0   NaN  178100
    3       NaN   NaN  140000
    

### 处理缺失值

&emsp;&emsp;典型的方法包括插值法和删除法， 其中插值法用一个替代值弥补缺失值，而删除法则直接忽略缺失值。


```python
inputs, outputs = data.iloc[:, 0:2], data.iloc[:, 2]
inputs = inputs.fillna(inputs.mean())
print(inputs)
```

       NumRooms Alley
    0       3.0  Pave
    1       2.0   NaN
    2       4.0   NaN
    3       3.0   NaN
    

    C:\Users\41961\AppData\Local\Temp\ipykernel_5672\38268100.py:2: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.
      inputs = inputs.fillna(inputs.mean())
    


```python
#对于inputs中的类别值或离散值，我们将“NaN”视为一个类别
inputs = pd.get_dummies(inputs, dummy_na=True)
print(inputs)
```

       NumRooms  Alley_Pave  Alley_nan
    0       3.0           1          0
    1       2.0           0          1
    2       4.0           0          1
    3       3.0           0          1
    

###  转换为张量格式


```python
X, y = torch.tensor(inputs.values), torch.tensor(outputs.values)
X, y
```




    (tensor([[3., 1., 0.],
             [2., 0., 1.],
             [4., 0., 1.],
             [3., 0., 1.]], dtype=torch.float64),
     tensor([127500, 106000, 178100, 140000]))



<br>
<br>

***

## 线性代数计算

### 标量


```python
import torch

x = torch.tensor(3.0)
y = torch.tensor(2.0)

x + y, x * y, x / y, x**y
```




    (tensor(5.), tensor(6.), tensor(1.5000), tensor(9.))



### 向量

&emsp;&emsp;你可以将向量视为标量值组成的列表。 我们将这些标量值称为向量的元素（element）或分量（component）。


```python
x = torch.arange(4)
x
```




    tensor([0, 1, 2, 3])




```python
x[3]
```




    tensor(3)




```python
len(x)
```




    4




```python
x.shape
```




    torch.Size([4])



### 矩阵

&emsp;&emsp;矩阵，我们通常用粗体、大写字母来表示, 在代码中表示为具有两个轴的张量。


```python
A = torch.arange(20).reshape(5, 4)
A
```




    tensor([[ 0,  1,  2,  3],
            [ 4,  5,  6,  7],
            [ 8,  9, 10, 11],
            [12, 13, 14, 15],
            [16, 17, 18, 19]])




```python
A.T
```




    tensor([[ 0,  4,  8, 12, 16],
            [ 1,  5,  9, 13, 17],
            [ 2,  6, 10, 14, 18],
            [ 3,  7, 11, 15, 19]])



### 张量

&emsp;&emsp;就像向量是标量的推广，矩阵是向量的推广一样，我们可以构建具有更多轴的数据结构。


```python
X = torch.arange(24).reshape(2, 3, 4)
X
```




    tensor([[[ 0,  1,  2,  3],
             [ 4,  5,  6,  7],
             [ 8,  9, 10, 11]],
    
            [[12, 13, 14, 15],
             [16, 17, 18, 19],
             [20, 21, 22, 23]]])




```python
A = torch.arange(20, dtype=torch.float32).reshape(5, 4)
B = A.clone()  # 通过分配新内存，将A的一个副本分配给B
A, A + B
```




    (tensor([[ 0.,  1.,  2.,  3.],
             [ 4.,  5.,  6.,  7.],
             [ 8.,  9., 10., 11.],
             [12., 13., 14., 15.],
             [16., 17., 18., 19.]]),
     tensor([[ 0.,  2.,  4.,  6.],
             [ 8., 10., 12., 14.],
             [16., 18., 20., 22.],
             [24., 26., 28., 30.],
             [32., 34., 36., 38.]]))




```python
A * B
```




    tensor([[  0.,   1.,   4.,   9.],
            [ 16.,  25.,  36.,  49.],
            [ 64.,  81., 100., 121.],
            [144., 169., 196., 225.],
            [256., 289., 324., 361.]])




```python
a = 2
X = torch.arange(24).reshape(2, 3, 4)
a + X, (a * X).shape
```




    (tensor([[[ 2,  3,  4,  5],
              [ 6,  7,  8,  9],
              [10, 11, 12, 13]],
     
             [[14, 15, 16, 17],
              [18, 19, 20, 21],
              [22, 23, 24, 25]]]),
     torch.Size([2, 3, 4]))



### 降维


```python
x = torch.arange(4, dtype=torch.float32)
x, x.sum()
```




    (tensor([0., 1., 2., 3.]), tensor(6.))




```python
A.shape, A.sum()
```




    (torch.Size([5, 4]), tensor(190.))




```python
A_sum_axis0 = A.sum(axis=0)
A_sum_axis0, A_sum_axis0.shape
```




    (tensor([40., 45., 50., 55.]), torch.Size([4]))




```python
A_sum_axis1 = A.sum(axis=1)
A_sum_axis1, A_sum_axis1.shape
```




    (tensor([ 6., 22., 38., 54., 70.]), torch.Size([5]))




```python
A.sum(axis=[0, 1])  # SameasA.sum()
```




    tensor(190.)




```python
#非降维求和
sum_A = A.sum(axis=1, keepdims=True)
sum_A
```




    tensor([[ 6.],
            [22.],
            [38.],
            [54.],
            [70.]])



### 点积


```python
y = torch.ones(4, dtype = torch.float32)
x, y, torch.dot(x, y)
```




    (tensor([0., 1., 2., 3.]), tensor([1., 1., 1., 1.]), tensor(6.))



### 矩阵-向量积


```python
A, x, torch.mv(A, x)
```




    (tensor([[ 0.,  1.,  2.,  3.],
             [ 4.,  5.,  6.,  7.],
             [ 8.,  9., 10., 11.],
             [12., 13., 14., 15.],
             [16., 17., 18., 19.]]),
     tensor([0., 1., 2., 3.]),
     tensor([ 14.,  38.,  62.,  86., 110.]))



### 矩阵-矩阵乘法


```python
B = torch.ones(4, 3)
A,B,torch.mm(A, B)
```




    (tensor([[ 0.,  1.,  2.,  3.],
             [ 4.,  5.,  6.,  7.],
             [ 8.,  9., 10., 11.],
             [12., 13., 14., 15.],
             [16., 17., 18., 19.]]),
     tensor([[1., 1., 1.],
             [1., 1., 1.],
             [1., 1., 1.],
             [1., 1., 1.]]),
     tensor([[ 6.,  6.,  6.],
             [22., 22., 22.],
             [38., 38., 38.],
             [54., 54., 54.],
             [70., 70., 70.]]))



### 范数


```python
#L2范式
u = torch.tensor([3.0, -4.0])
torch.norm(u)
```




    tensor(5.)




```python
torch.abs(u).sum()
```




    tensor(7.)



<br>
<br>

***

## 微积分/自动微分

逼近法就是积分（integral calculus）的起源

&emsp;&emsp;我们“训练”模型，不断更新它们，使它们在看到越来越多的数据时变得越来越好。 通常情况下，变得更好意味着最小化一个损失函数（loss function）， 即一个衡量“我们的模型有多糟糕”这个问题的分数。 最终，我们真正关心的是生成一个模型，它能够在从未见过的数据上表现良好。 但“训练”模型只能将模型与我们实际能看到的数据相拟合。 因此，我们可以将拟合模型的任务分解为两个关键问题：

- 优化（optimization）：用模型拟合观测数据的过程；

- 泛化（generalization）：数学原理和实践者的智慧，能够指导我们生成出有效性超出用于训练的数据集本身的模型。

### 例子


```python
import torch

x = torch.arange(4.0)
x
```




    tensor([0., 1., 2., 3.])




```python
x.requires_grad_(True)  # 等价于x=torch.arange(4.0,requires_grad=True)
x.grad  # 默认值是None
```


```python
y = 2 * torch.dot(x, x)
y
```




    tensor(28., grad_fn=<MulBackward0>)




```python
# 通过调用反向传播函数来自动计算y关于x每个分量的梯度，并打印这些梯度。
y.backward()
x.grad
```




    tensor([ 0.,  4.,  8., 12.])




```python
x.grad == 4 * x
```




    tensor([True, True, True, True])




```python
# 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值
x.grad.zero_()
y = x.sum()
y.backward()
x.grad
```




    tensor([1., 1., 1., 1.])



### 分离计算

&emsp;&emsp;有时，我们希望将某些计算移动到记录的计算图之外。 例如，假设y是作为x的函数计算的，而z则是作为y和x的函数计算的。如果想计算z关于x的梯度，但由于某种原因，我们希望将y视为一个常数， 并且只考虑到x在y被计算后发挥的作用。

在这里，我们可以分离y来返回一个新变量u，该变量与y具有相同的值， 但丢弃计算图中如何计算y的任何信息。 换句话说，梯度不会向后流经u到x。 因此，下面的反向传播函数计算z=u*x关于x的偏导数，同时将u作为常数处理， 而不是z=x*x*x关于x的偏导数。


```python
x.grad.zero_()
y = x * x
u = y.detach()
z = u * x

z.sum().backward()
x.grad == u
```




    tensor([True, True, True, True])




```python
x.grad.zero_()
y.sum().backward()
x.grad == 2 * x
```




    tensor([True, True, True, True])



### Python控制流的梯度计算

&emsp;&emsp;使用自动微分的一个好处是： 即使构建函数的计算图需要通过Python控制流（例如，条件、循环或任意函数调用），我们仍然可以计算得到的变量的梯度。 在下面的代码中，while循环的迭代次数和if语句的结果都取决于输入a的值。


```python
def f(a):
    b = a * 2
    while b.norm() < 1000:
        b = b * 2
    if b.sum() > 0:
        c = b
    else:
        c = 100 * b
    return c
```


```python
a = torch.randn(size=(), requires_grad=True)
d = f(a)
d.backward()
```


```python
a.grad, a.grad == d / a
```




    (tensor(204800.), tensor(True))





<br>
<br>

***

## 概率


```python
import torch
from torch.distributions import multinomial
import matplotlib.pyplot as plt
```

### 基本概率论


```python
fair_probs = torch.ones([6]) / 6
multinomial.Multinomial(1, fair_probs).sample()
```




    tensor([0., 0., 1., 0., 0., 0.])




```python
#在估计一个骰子的公平性时，我们希望从同一分布中生成多个样本。 如果用Python的for循环来完成这个任务，速度会慢得惊人。 因此我们使用深度学习框架的函数同时抽取多个样本，得到我们想要的任意形状的独立样本数组。
multinomial.Multinomial(10, fair_probs).sample()
```




    tensor([1., 2., 0., 4., 2., 1.])




```python
counts = multinomial.Multinomial(10, fair_probs).sample((500,))
cum_counts = counts.cumsum(dim=0)
estimates = cum_counts / cum_counts.sum(dim=1, keepdims=True)

plt.figure(figsize=(20,10))
for i in range(6):
    plt.plot(estimates[:, i].numpy(),
                 label=("P(die=" + str(i + 1) + ")"))
plt.axhline(y=0.167, color='black', linestyle='dashed')
plt.gca().set_xlabel('Groups of experiments')
plt.gca().set_ylabel('Estimated probability')
plt.legend();
```


    
![png](01_torch%E5%85%A5%E9%97%A8_files/01_torch%E5%85%A5%E9%97%A8_103_0.png)
    


### 处理多个随机变量


```python

```
