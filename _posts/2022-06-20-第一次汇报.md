---
layout: post
title: 2022-06-20-第一次汇报 
tags: [cv,ml]
category: CV
toc: true
math: true
author: zzhc
---


## 19_在实践中推广行人重识别

&emsp;&emsp;浅层中的实例规范化（IN）过滤掉了样式统计变化，而深层中的特征规范化（FN）能够进一步消除内容统计中的差异。 


### 1 前置知识

 - resnet
 - MobileNetV2
 - 各种标准化

![enter description here](http://img.zzhc321.xyz/blog/1655722656639.png)


#### 1.3 各种标准化
&emsp;&emsp;协变量可以看作是 输入变量。一般的深度神经网络都要求输入变量在训练数据和测试数据上的分布是相似的，这是通过训练数据获得的模型能够在测试集获得好的效果的一个基本保障。传统的深度神经网络在训练时，随着参数的不算更新，中间每一层输入的数据分布往往会和参数更新之前有较大的差异，导致网络要去不断的适应新的数据分布，进而使得训练变得异常困难，这种现象叫做内部协变量偏移(internal Covariate Shift)。

&emsp;&emsp;为了解决这个问题，Sergey Ioffe’s 和 Christian Szegedy’s 在2015年首次提出了批量标准化（Batch Normalization，BN）的想法。该想法是：不仅仅对输入层做标准化处理，还要对 每一中间层的输入(激活函数前) 做标准化处理，使得输出服从均值为 0，方差为 1 的正态分布，从而避免内部协变量偏移的问题。之所以称之为 批 标准化：是因为在训练期间，我们仅通过计算 当前层一小批数据 的均值和方差来标准化每一层的输入。

 - 减少了内部神经元分布的改变，使降低了不同样本间值域的差异性，得大部分的数据都其处在非饱和区域，从而保证了梯度能够很好的回传，避免了梯度消失和梯度爆炸
 - 通过减少梯度对参数或其初始值尺度的依赖性，使得我们可以使用较大的学习速率对网络进行训练，从而加速网络的收敛
 - 由于在训练的过程中批量标准化所用到的均值和方差是在一小批样本(mini-batch)上计算的，而不是在整个数据集上，所以均值和方差会有一些小噪声产生，可以看作是一种正则化手段，提高了网络的泛化能力，使得我们可以减少或者取消 Dropout，优化网络结构。
 - BN 通常应用于输入层或任意中间层，且最好在激活函数前使用。
 - 使用 BN 的层不需要加 bias 项了
   

#### 1.4 IN、FN

 - BN 和 IN 其实本质上是同一个东西，只是 IN 是作用于单张图片（对单个图片的所有像素求均值和标准差）
 - FN 是 BN 的特例，只需令 BN 中的缩放和平移参数 β = 0 \beta=0β=0、γ = 1 \gamma=1γ=1 即可，FN 服从标准正态分布。
   

  
<br>



### 2 引言
&emsp;&emsp;目前大多数的重新识别方法都过于适合特定的数据集（摄像头网络），因为一旦在给定的数据集上进行了训练，如果应用于不同的摄像头网络，它们的性能就会很差。 

&emsp;&emsp;虽然UDA方法降低了样本标注成本，但它们仍然需要在模型自适应训练之前收集目标网络的数据。

&emsp;&emsp;DG在训练时看不到目标域数据。DualNorm解决方案通过深度特征提取中的归一化明确解决了域偏差问题。设计了一个CNN网络结构，该结构共同规范了风格和内容统计：在浅层中的每个瓶颈处使用实例规范化（IN）层来捕获和消除样式变化，再通过对提取的特征执行基于批次标准化（BN）的标准化，对不同的内容统计数据进行标准化。通过联合处理风格和内容差异以明确解决领域偏差，提取的特征更具普遍性和领域不可知性。 


  
<br>



### 3 相关工作

 - **域自适应与泛化**：Unsupervised Domain Adaptation (UDA)
 - **标准化**：实例规范化（IN）在单个样本上执行类似Batch Normalization的计算。 
 - **与之前不同**：

  
<br>


### 4 方法

![enter description here](http://img.zzhc321.xyz/blog/1655723563631.png)

&emsp;&emsp;保留了MobileNetV2的默认结构，只是将最后一个分类层的维度更改为N，即标识的总数。我们使用交叉熵损失来计算所有源域的损失： 


#### 4.1 风格标准化


&emsp;&emsp;实例规范化（IN）[44]，主要用于样式转换任务。通过标准化原始bottlenecks的输出，我们过滤掉特定于实例的风格差异，这使得学习的特征更不受数据集的影响。 

&emsp;&emsp;根据文献*Two at once: Enhancing learning and generalization capacities via ibn-net.*中的发现，风格差异引起的外观差异主要存在于浅层，在浅层中加入可以有效减少风格差异引起的领域转移。另一方面，应用于深层可能会导致鉴别信息的严重丢失，并降低分类性能。因此，仅为网络的浅层添加。



#### 4.2 内容规范化

&emsp;&emsp;通过引入l2范数的平滑度来帮助特征鲁棒性，这也是BN的基本效果。N使用在小批量中计算的平均µ和方差σ2对f（xn）的每个通道进行标准化。它使梯度更稳定，并实现更高的学习率，从而产生更快的收敛和更好的泛化。 





  
<br>




### 5 结论
&emsp;&emsp;通过实例和特征标准化的结合，减少了领域之间的风格和内容偏差，提高了深度Re-ID的泛化和可转移性。




























<br>
<br>


***

## 20_Learning to Optimize Domain Specific Normalization for Domain Generalization 

&emsp;&emsp;提出了一种简单而有效的基于深度神经网络的多源领域泛化技术，该技术结合了特定于各个领域的优化归一化层。
### 1 前置知识

<br>

### 2 引言

&emsp;&emsp;领域泛化技术根据其方法分为不同组。一些算法定义了新的损失函数、一些设计深层神经网络架构。

&emsp;&emsp;一种简单的方法是，使用所有训练示例（无论其域成员身份如何）训练一个具有批量归一化的单个深层神经网络。然而，当域偏移显著时，批标准化的好处是有限的，通常需要删除特定于域的样式以更好地进行泛化。实例规范证明是实现这一目标的有效方案，通过数据驱动的两种规范化方法的平衡，将批处理和实例规范化技术结合起来，进一步提高了准确性。


&emsp;&emsp;DSON算法的目标是优化每个域中标准化技术的组合，而不同的域为混合标准化学习不同的参数。可以通过控制标准化层中的normalization类型和参数来学习域不变表征。但是所有其他参数，包括卷积层中的参数，都是跨域共享的。着眼于如何在不丢失语义的情况下删除style信息，以便在不可见的域上进行泛化。


<br>

### 3 相关工作

 - Domain Generalization
 - Multi-Source Domain Adaptation：多源域自适应可以看作是介于域自适应和泛化之间的。
 - Normalization in Neural Networks：最初设计用于正则化训练模型并提高其泛化性能。



   <br>
   
   
### 4 DSON 
多个源域学习一个分类器，分类目标域中的类别。从每个源域训练域不变分类器并集成其预测（留一验证）。

loss function：
![enter description here](http://img.zzhc321.xyz/blog/1655886424538.png)




&emsp;&emsp;BN在存在较大领域差异的情况下进行训练时，其性能会持续下降。这是因为批处理统计数据过度适合特定的训练域，导致在不可见的域中泛化性能较差。这促使我们构建一个领域不可知的特征提取器。

&emsp;&emsp;使用IN作为减少每个域中固有样式信息的一种方法。然而，IN的缺点是，它使特征相对于对象类别的区分性降低。通过优化跨类别方差和域不变性之间的权衡，使用IN和BN的混合。更具体地说，通过线性插值两个标准化统计量的均值和方差，将其融合到网络的所有BN层中。

![enter description here](http://img.zzhc321.xyz/blog/1655891046066.png)


















<br>
<br>


***

## 2020_Style Normalization and Restitution for Generalizable Person Re-identification 

&emsp;&emsp;通过实例规范化（IN）过滤掉样式变化（例如，照明、颜色对比度）。然而，这样的过程不可避免地会删除歧视性信息。我们建议从删除的信息中提取与身份相关的特征，并将其恢复到网络中，以确保高度的歧视性。


<br>

### 1 前置知识

 - 通道（Channel）：也有叫特征图（feature map）RBG...

<br>

### 2 引言

&emsp;&emsp;域/数据集之间通常存在样式差异，这阻碍了高泛化能力的实现。从残差信息（即原始信息和标准化化信息之间的差异）中，我们进一步提取身份相关信息，作为对归一化信息的补偿。

 - SNR简单而有效，可以作为现有ReID体系结构的即插即用模块，以增强其泛化能力。
 - 在SNR中引入了双重因果关系损失约束，以便更好地进行特征分离。


<br>

### 3 相关工作


 - Supervised Person ReID.
 - Unsupervised Domain Adaptation (UDA) for Person ReID.
 - Domain Generalization (DG).



<br>

### 4 可泛化的行人重识别



#### 4.1 Style Normalization and Restitution (SNR)

 - Style Normalization Phase.
 - Style Restitution Phase.
 - Dual Causality Loss Constraint.


#### 4.3 joint training

使用ResNet-50作为基本ReID网络，并在每个卷积块（共四个卷积块/级）后插入SNR模块。
![enter description here](http://img.zzhc321.xyz/blog/1655965278018.png)



















<br>


<br>


***


## 21_Meta Batch-Instance Normalization for Generalizable Person Re-Identification


&emsp;&emsp;将可学习的批处理实例规范化层与元学习相结合

### 1 前置知识

 - 元学习


meata-training所有任务共享初始参数θ，学习一个好的初始化参数（提供一个指导方向），避免meta-testing新任务从头开始学（微调![enter description here](https://markdown.xiaoshujiang.com/img/spinner.gif "[[[1656153704453]]]" )）。

<br>

### 2 引言

&emsp;&emsp;SNR模块。由于该方法的目的是仅从给定的源域中消除样式差异，因此它缺乏在不可见域上减少新风格的能力。

&emsp;&emsp;由于两种标准化方法在DG设置中都有局限性，因此必须仔细处理BN和IN的组合。Meta Batch Instance Normalization（MetaBIN），用于学习通用化标准化层。关键思想是模拟前面提到的不成功的泛化场景


<br>

### 3 相关工作

 - Generalizable person re-identification
 - Batch-instance normalization
 - Model-agnostic meta-learning for DG


<br>

### 4 方法

&emsp;&emsp;在训练阶段，我们使用所有源域的聚合图像标签对来训练DG模型。在测试阶段，我们在看不见的目标域上执行检索任务，而无需额外的模型更新。


#### 4.1 Batch-Instance Normalization

![enter description here](http://img.zzhc321.xyz/blog/1656062160264.png)

 ˆx is the normalized response by mean and variance,

#### 4.2 Meta Batch-Instance Normalization


使用元学习管道开发了我们的标准化模型。

cross-entropy loss with the triplet loss
























<br>


<br>


***


## 21_Learning to Generalize Unseen Domains via Memory-based Multi-Source Meta-Learning for Person Re-Identification


&emsp;&emsp;引入元学习策略来模拟领域泛化的训练测试过程，以学习更泛化的模型。为了克服由参数分类器引起的不稳定元优化，我们提出了一种基于记忆的非参数识别丢失方法，该方法与元学习相协调。我们还提出了一个元批处理规范化层（MetaBN），以使元测试特性多样化，进一步确立元学习的优势。

### 1 前置知识

 - 温度因子

![enter description here](http://img.zzhc321.xyz/blog/1656327853881.png)


<br>

### 2 引言

meta-train被视为源数据，meta-test被视为“看不见”的数据。提出了一种基于记忆的识别丢失方法，它使用非参数记忆来充分利用元学习，同时避免不稳定的优化。我们还引入了元批量规范化层（MetaBN），该层将元训练知识与元测试特征混合，以模拟不同领域中的特征变化。


<br>

### 3 相关工作

 - Person Re-identification
 - Domain Generalization
 - Meta Learning



<br>

### 4 方法


#### 4.1 Memory-based Identification Loss

####





























<br>
<br>



***



## 21_基于MIXSTYLE的域泛化 
&emsp;&emsp;卷积神经网络（CNN）在学习区分特征方面有卓越的能力，但通常很难推广到不可见的领域。领域泛化旨在通过从一组源领域学习一个可泛化到任何不可见领域的模型来解决这个问题。本文提出了一种基于跨源域训练样本瞬时级特征概率混合统计的新方法（Mixstyle）。

### 1 前置知识

 - AdaIN.
 - style transfer
 - mixup





<br>

***

### 2 引言


&emsp;&emsp;理解数字图像的关键是计算一个紧凑且信息丰富的特征表示。CNN严重依赖i.i.d假设（训练测试是同一个分布）。

&emsp;&emsp;领域综合（DG）旨在解决这一问题，DG模型必须依赖于源域，并专注于学习域不变的特征表示，以期在给定目标域数据的情况下保持区分性。然而，收集各种领域的数据往往成本高昂，甚至不可能。因此，它不可能是DG的通用解决方案。 















## 附读：

 - Generalizable person re-identification by domain-invariant mapping network. In CVPR, 2019.

















