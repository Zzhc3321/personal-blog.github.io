---
layout: post
title: 2022-06-20-第一次汇报 
tags: [cv,ml]
category: CV
toc: true
math: true
author: zzhc
---


## 19_在实践中推广行人重识别

&emsp;&emsp;浅层中的实例规范化（IN）过滤掉了样式统计变化，而深层中的特征规范化（FN）能够进一步消除内容统计中的差异。 


### 1 前置知识

 - resnet
 - MobileNetV2
 - 各种标准化

![enter description here](http://img.zzhc321.xyz/blog/1655722656639.png)


#### 1.3 各种标准化
&emsp;&emsp;协变量可以看作是 输入变量。一般的深度神经网络都要求输入变量在训练数据和测试数据上的分布是相似的，这是通过训练数据获得的模型能够在测试集获得好的效果的一个基本保障。传统的深度神经网络在训练时，随着参数的不算更新，中间每一层输入的数据分布往往会和参数更新之前有较大的差异，导致网络要去不断的适应新的数据分布，进而使得训练变得异常困难，这种现象叫做内部协变量偏移(internal Covariate Shift)。
&emsp;&emsp;为了解决这个问题，Sergey Ioffe’s 和 Christian Szegedy’s 在2015年首次提出了批量标准化（Batch Normalization，BN）的想法。该想法是：不仅仅对输入层做标准化处理，还要对 每一中间层的输入(激活函数前) 做标准化处理，使得输出服从均值为 0，方差为 1 的正态分布，从而避免内部协变量偏移的问题。之所以称之为 批 标准化：是因为在训练期间，我们仅通过计算 当前层一小批数据 的均值和方差来标准化每一层的输入。

 - 减少了内部神经元分布的改变，使降低了不同样本间值域的差异性，得大部分的数据都其处在非饱和区域，从而保证了梯度能够很好的回传，避免了梯度消失和梯度爆炸
 - 通过减少梯度对参数或其初始值尺度的依赖性，使得我们可以使用较大的学习速率对网络进行训练，从而加速网络的收敛
 - 由于在训练的过程中批量标准化所用到的均值和方差是在一小批样本(mini-batch)上计算的，而不是在整个数据集上，所以均值和方差会有一些小噪声产生，可以看作是一种正则化手段，提高了网络的泛化能力，使得我们可以减少或者取消 Dropout，优化网络结构。
 - BN 通常应用于输入层或任意中间层，且最好在激活函数前使用。
 - 使用 BN 的层不需要加 bias 项了
   

#### 1.4 IN、FN

 - BN 和 IN 其实本质上是同一个东西，只是 IN 是作用于单张图片（对单个图片的所有像素求均值和标准差）
 - N 是 BN 的特例，只需令 BN 中的缩放和平移参数 β = 0 \beta=0β=0、γ = 1 \gamma=1γ=1 即可，FN 服从标准正态分布。
   

  
<br>

***

### 2 引言
&emsp;&emsp;目前大多数的重新识别方法都过于适合特定的数据集（摄像头网络），因为一旦在给定的数据集上进行了训练，如果应用于不同的摄像头网络，它们的性能就会很差。 

&emsp;&emsp;虽然UDA方法降低了样本标注成本，但它们仍然需要在模型自适应训练之前收集目标网络的数据。

&emsp;&emsp;DG在训练时看不到目标域数据。DualNorm解决方案通过深度特征提取中的归一化明确解决了域偏差问题。设计了一个CNN网络结构，该结构共同规范了风格和内容统计：在浅层中的每个瓶颈处使用实例规范化（IN）层来捕获和消除样式变化，再通过对提取的特征执行基于批次标准化（BN）的标准化，对不同的内容统计数据进行标准化。通过联合处理风格和内容差异以明确解决领域偏差，提取的特征更具普遍性和领域不可知性。 


  
<br>

***

### 3 相关工作

 - **域自适应与泛化**：Unsupervised Domain Adaptation (UDA)
 - **标准化**：实例规范化（IN）在单个样本上执行类似Batch Normalization的计算。 
 - **与之前不同**：

  
<br>

***

### 4 方法

![enter description here](http://img.zzhc321.xyz/blog/1655723563631.png)

&emsp;&emsp;保留了MobileNetV2的默认结构，只是将最后一个分类层的维度更改为N，即标识的总数。我们使用交叉熵损失来计算所有源域的损失： 


#### 4.1 风格标准化


&emsp;&emsp;实例规范化（IN）[44]，主要用于样式转换任务。通过标准化原始bottlenecks的输出，我们过滤掉特定于实例的风格差异，这使得学习的特征更不受数据集的影响。 

&emsp;&emsp;根据文献*Two at once: Enhancing learning and generalization capacities via ibn-net.*中的发现，风格差异引起的外观差异主要存在于浅层，在浅层中加入可以有效减少风格差异引起的领域转移。另一方面，应用于深层可能会导致鉴别信息的严重丢失，并降低分类性能。因此，仅为网络的浅层添加。



#### 4.2 内容规范化

&emsp;&emsp;通过引入l2范数的平滑度来帮助特征鲁棒性，这也是BN的基本效果。N使用在小批量中计算的平均µ和方差σ2对f（xn）的每个通道进行标准化。它使梯度更稳定，并实现更高的学习率，从而产生更快的收敛和更好的泛化。 





  
<br>

***


### 5 结论
&emsp;&emsp;通过实例和特征标准化的结合，减少了领域之间的风格和内容偏差，提高了深度Re-ID的泛化和可转移性。





<br>
<br>

***

***


## 19_在实践中推广行人重识别


















## 附读：

 - Generalizable person re-identification by domain-invariant mapping network. In CVPR, 2019.

















