---
layout: post
title: 2022-06-12-机器学习/深度学习知识点 
tags: [cv,ML]
category: 深度学习
toc: true
math: true
author: zzhc
---



## 1 特征工程

&emsp;&emsp;对原始数据进行一系列工程处理，将其提炼为特征，作为输入供算法和模型使用。特征工程旨在去除原始数据中的杂质和冗余，设计更高效的特征以刻画求解的问题与预测模型之间的关系。


### 1.1 特征归一化

&emsp;&emsp;为了消除数据特征之间的量纲影响，我们需要对特征进行归一化处理，使得不同指标之间具有可比性。在*学习率*相同的情况下，大的值需要更多到迭代才能找到最优解。

 - **线性函数归一化**（Min-Max Scaling）->[0, 1]
 - **零均值归一化**（Z-Score Normalization）



<br>

### 1.2 类别型特征
&emsp;&emsp;除了决策树等少数模型能直接处理字符串形式的输入，对于逻辑回归、支持向量机等模型来说，类别型特征必须经过处理转换成数值型特征才能正确工作。

 - **序号编码**：高表示为3、中表示为2、低表示为1，转换后依然保留了大小关系。
 - **独热编码**：通常用于处理类别间不具有大小关系的特征
 - **二进制编码**：ID为1，二进制表示为001；ID为2，二进制表示为
010。



<br>
### 1.3 高维组合特征的处理

&emsp;&emsp;为了提高复杂关系的拟合能力，在特征工程中经常会把一阶离散特征两两组合，构成高阶组合特征。有一种基于决策树的特征组合寻找方法，每一条从根节点到叶节点的路径都可以看成一种特征组合的方式。



<br>
### 1.4 文本表示模型

 - 词袋模型和N-gram模型： 将整段文本以词为单位切分开，然后每篇文章可以表示成一个长向量，向量中的每一维代表一个单词，而该维对应的权重则反映了这个词在原文章中的重要程度。常用TF-IDF来计算权重，其中TF(t,d)为单词t在文档d中出现的频率，IDF(t)是逆文档频率，用来衡量单词t对表达语义所起的重要性。直观的解释是，如果一个单词在非常多的文章里面都出现，那么它可能是一个比较通用的词汇，对于区分某篇文章特殊语义的贡献较小，因此对权重做一定惩罚。
 - 主题模型：主题模型用于从文本库中发现有代表性的主题
 - 词嵌入与深度学习模型：核心思想是将每个词都映射成低维空间（通常K=50～300维）上的一个稠密向量（Dense Vector）



<br>
### 1.5 Word2Vec

 - **CBOW**的目标是根据上下文出现的词语来预测当前词的生成概率
 - **Skip-gram**是根据当前词来预测上下文中各词的生成概率。



<br>
### 1.6 图像数据不足时的处理方法

 - 迁移学习（Transfer Learning）
 - 生成对抗网络
 - 图像处理
 - 上采样技术
 - 数据扩充

<br>
<br>

***

## 2 模型评估

### 2.1 评估指标的局限性

&emsp;&emsp;分类问题、排序问题、回归问题往往需要使用不同的指标进行评估。在诸多的评估指标中，大部分指标只能片面地反映模型的一部分性能。如果不能合理地运用评估指标，不仅不能发现模型本身的问题，而且会得出错误的结论。

 1. **准确率(Accuracy)**：正确数/总数（样本不均衡时）
 2. **精确率(Precision)**：正样本中预测正确个数/样本总数
 3. **召回率(Recall)**：正样本中预测正确个数/正样本个数
 4. **均方根误差(Root Mean Square Error)**：平均绝对百分比误差
 5. **F1-Score** = 2×precision×recall/(precision+recall)：只有通过P-R曲线的整体表现，才能够对模型进行更为全面的评估



<br>
### 2.2 ROC曲线（“受试者工作特征曲线”）

&emsp;&emsp;ROC曲线的横坐标为假阳性率（False Positive Rate，FPR）；纵坐标为真阳性率（True Positive Rate，TPR）。P是真实的正样本的数量，N是真实的负样本的数量，TP是P个正样本中被分类器预测为正样本的个数，FP是N个负样本中被分类器预测为正样本的个数。

 1. **FPR = FP/N**
 2. **TPR = TP/P**


&emsp;&emsp;AUC指的是ROC曲线下的面积大小，该值能够量化地反映基于ROC曲线衡量出的模型性能。AUC的取值一般在0.5～1之间。AUC越大，说明分类器越可能把真正的正样本排在前面，分类性能越好。
&emsp;&emsp;当正负样本的分布发生变化时，ROC曲线的形状能够基本保持不变，而P-R曲线的形状一般会发生较剧烈的变化。


<br>
### 2.3 余弦距离

&emsp;&emsp;通常将特征表示为向量的形式，所以在分析两个特征向量之间的相似性时，常使用余弦相似度来表示。余弦相似度的取值范围是[−1,1]，相同的两个向量之间的相似度为1。

&emsp;&emsp;使得三条距离公理（正定性，对称性，三角不等式）成立，则该实数可称为这对元素之间的距离。余弦距离满足正定性和对称性，但是不满足三角不等式，因此它并不是严格定义的距离。KL距离（Kullback-Leibler Divergence），也叫作相对熵，它常用于计算两个分布之间的差异，但不满足对称性和三角不等式。

<br>
### 2.4 模型评估的方法

 - **Holdout检验**：将原始的样本集合随机划分成训练集和验证集两部分。
 - **交叉检验**：k-fold交叉验证：首先将全部样本划分成k个大小相等的样本子集；依次遍历这k个子集，每次把当前子集作为验证集，其余所有子集作为训练集，进行模型的训练和评估；最后把k次评估指标的平均值作为最终的评估指标。
 - **自助法**：对于总数为n的样本集合，进行n次有放回的随机抽样，得到大小为n的训练集。n次采样过程中，有的样本会被重复采样，有的样本没有被抽出过，将这些没有被抽出的样本作为验证集，进行模型验证。



<br>
### 2.5 超参数调优

 - **网格搜索**
 - **随机搜索**：随机搜索一般会比网格搜索要快一些，但是和网格搜索的快速版一样，它的结果也是没法保证的。
 - **贝叶斯优化算法**：贝叶斯优化算法则充分利用了之前的信息。贝叶斯优化算法通过对目标函数形状进行学习，找到使目标函数向全局最优值提升的参数。


<br>
### 2.6 过拟合与欠拟合

 - **过拟合**：获得更多的训练数据，降低模型复杂度，正则化方法，集成学习方法。
 - **欠拟合**：添加新特征，增加模型复杂度，减小正则化系数。






<br>
<br>

***

## 3 经典算法

### 3.1 支持向量机

 1. 对于任意线性可分的两组点，它们在SVM分类的超平面上的投影都是线性不可分的
 2. 若给定训练集中不存在两个点在同一位置，则存在一组参数{α1,...,αm,b}以及参数γ使得该SVM的训练误差为0
 3. 使用SMO算法训练的线性分类器并不一定能得到训练误差为0的模型，这是由于我们的优化目标改变了，并不再是使训练误差最小。
    


<br>
### 3.2 逻辑回归

&emsp;&emsp;一个事件的几率（odds）定义为该事件发生的概率与该事件不发生的概率的比值，那么逻辑回归可以看作是对于“y=1|x”这一事件的对数几率的线性回归。

**多项逻辑回归（Softmax Regression）**


<br>
### 3.3 决策树

 - C4.5——最大信息增益比
 - CART——最大基尼指数（Gini）


&emsp;&emsp;决策树的剪枝通常有两种方法，预剪枝（Pre-Pruning）和后剪枝（PostPruning）。预剪枝，即在生成决策树的过程中提前停止树的增长。而后剪枝，是在已生成的过拟合决策树上进行剪枝，得到简化版的剪枝决策树。


<br>
<br>

***

## 4 降维
### 4.1 PCA

&emsp;&emsp;此向量xi在ω（单位方向向量）上的投影坐标可以表示为xi·ω 。
&emsp;&emsp;x投影后的方差就是协方差矩阵的
特征值。我们要找到最大的方差也就是协方差矩阵最大的特征值，最佳投影方向
就是最大特征值所对应的特征向量。










<br>
<br>

***

## 5 非监督学习















<br>
<br>

***

## 6 概率图模型

















<br>
<br>

***

## 7 优化算法

















<br>
<br>

***

## 8 采样

















<br>
<br>

***

## 9 前向神经网络

















<br>
<br>

***

## 10 循环神经网络

















<br>
<br>

***

## 11 强化学习

















<br>
<br>

***

## 12 集成学习

















<br>
<br>

***

## 13 GAN













































<br>
<br>

***

## 14 卷积神经网络




























<br>
<br>

***

## 15 循环神经网络

















<br>
<br>

***

## 16 图神经网络













<br>
<br>

***

## 





















<br>
<br>

***

## 
















<br>
<br>

***

## 















<br>
<br>

***

## 










<br>
<br>

***

## 














<br>
<br>

***

## 










<br>
<br>

***

## 











<br>
<br>

***

## 











<br>
<br>

***

## 











<br>
<br>

***

## 












<br>
<br>

***

## 















<br>
<br>

***

## 












<br>
<br>

***

## 













<br>
<br>

***

## 















<br>
<br>

***

## 



















<br>
<br>

***

## 
















<br>
<br>

***

## 

















<br>
<br>

***

## 




















<br>
<br>

***

## 















<br>
<br>

***

## 

















