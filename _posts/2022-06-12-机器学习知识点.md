---
layout: post
title: 2022-06-12-机器学习/深度学习知识点 
tags: [cv,ML]
category: 深度学习
toc: true
math: true
author: zzhc
---



## 1 特征工程

&emsp;&emsp;对原始数据进行一系列工程处理，将其提炼为特征，作为输入供算法和模型使用。特征工程旨在去除原始数据中的杂质和冗余，设计更高效的特征以刻画求解的问题与预测模型之间的关系。


### 1.1 特征归一化

&emsp;&emsp;为了消除数据特征之间的量纲影响，我们需要对特征进行归一化处理，使得不同指标之间具有可比性。在*学习率*相同的情况下，大的值需要更多到迭代才能找到最优解。

 - **线性函数归一化**（Min-Max Scaling）->[0, 1]
 - **零均值归一化**（Z-Score Normalization）



<br>

### 1.2 类别型特征
&emsp;&emsp;除了决策树等少数模型能直接处理字符串形式的输入，对于逻辑回归、支持向量机等模型来说，类别型特征必须经过处理转换成数值型特征才能正确工作。

 - **序号编码**：高表示为3、中表示为2、低表示为1，转换后依然保留了大小关系。
 - **独热编码**：通常用于处理类别间不具有大小关系的特征
 - **二进制编码**：ID为1，二进制表示为001；ID为2，二进制表示为
010。



<br>
### 1.3 高维组合特征的处理

&emsp;&emsp;为了提高复杂关系的拟合能力，在特征工程中经常会把一阶离散特征两两组合，构成高阶组合特征。有一种基于决策树的特征组合寻找方法，每一条从根节点到叶节点的路径都可以看成一种特征组合的方式。



<br>
### 1.4 文本表示模型

 - 词袋模型和N-gram模型： 将整段文本以词为单位切分开，然后每篇文章可以表示成一个长向量，向量中的每一维代表一个单词，而该维对应的权重则反映了这个词在原文章中的重要程度。常用TF-IDF来计算权重，其中TF(t,d)为单词t在文档d中出现的频率，IDF(t)是逆文档频率，用来衡量单词t对表达语义所起的重要性。直观的解释是，如果一个单词在非常多的文章里面都出现，那么它可能是一个比较通用的词汇，对于区分某篇文章特殊语义的贡献较小，因此对权重做一定惩罚。
 - 主题模型：主题模型用于从文本库中发现有代表性的主题
 - 词嵌入与深度学习模型：核心思想是将每个词都映射成低维空间（通常K=50～300维）上的一个稠密向量（Dense Vector）



<br>
### 1.5 Word2Vec

 - **CBOW**的目标是根据上下文出现的词语来预测当前词的生成概率
 - **Skip-gram**是根据当前词来预测上下文中各词的生成概率。



<br>
### 1.6 图像数据不足时的处理方法

 - 迁移学习（Transfer Learning）
 - 生成对抗网络
 - 图像处理
 - 上采样技术
 - 数据扩充

<br>
<br>

***

## 2 模型评估

### 2.1 评估指标的局限性

&emsp;&emsp;分类问题、排序问题、回归问题往往需要使用不同的指标进行评估。在诸多的评估指标中，大部分指标只能片面地反映模型的一部分性能。如果不能合理地运用评估指标，不仅不能发现模型本身的问题，而且会得出错误的结论。

 1. **准确率(Accuracy)**：正确数/总数（样本不均衡时）
 2. **精确率(Precision)**：正样本中预测正确个数/样本总数
 3. **召回率(Recall)**：正样本中预测正确个数/正样本个数
 4. **均方根误差(Root Mean Square Error)**：平均绝对百分比误差
 5. **F1-Score** = 2×precision×recall/(precision+recall)：只有通过P-R曲线的整体表现，才能够对模型进行更为全面的评估



<br>
### 2.2 ROC曲线（“受试者工作特征曲线”）

&emsp;&emsp;ROC曲线的横坐标为假阳性率（False Positive Rate，FPR）；纵坐标为真阳性率（True Positive Rate，TPR）。P是真实的正样本的数量，N是真实的负样本的数量，TP是P个正样本中被分类器预测为正样本的个数，FP是N个负样本中被分类器预测为正样本的个数。

 1. **FPR = FP/N**
 2. **TPR = TP/P**


&emsp;&emsp;AUC指的是ROC曲线下的面积大小，该值能够量化地反映基于ROC曲线衡量出的模型性能。AUC的取值一般在0.5～1之间。AUC越大，说明分类器越可能把真正的正样本排在前面，分类性能越好。
&emsp;&emsp;当正负样本的分布发生变化时，ROC曲线的形状能够基本保持不变，而P-R曲线的形状一般会发生较剧烈的变化。


<br>
### 2.3 余弦距离

&emsp;&emsp;通常将特征表示为向量的形式，所以在分析两个特征向量之间的相似性时，常使用余弦相似度来表示。余弦相似度的取值范围是[−1,1]，相同的两个向量之间的相似度为1。

&emsp;&emsp;使得三条距离公理（正定性，对称性，三角不等式）成立，则该实数可称为这对元素之间的距离。余弦距离满足正定性和对称性，但是不满足三角不等式，因此它并不是严格定义的距离。KL距离（Kullback-Leibler Divergence），也叫作相对熵，它常用于计算两个分布之间的差异，但不满足对称性和三角不等式。

<br>
### 2.4 模型评估的方法

 - **Holdout检验**：将原始的样本集合随机划分成训练集和验证集两部分。
 - **交叉检验**：k-fold交叉验证：首先将全部样本划分成k个大小相等的样本子集；依次遍历这k个子集，每次把当前子集作为验证集，其余所有子集作为训练集，进行模型的训练和评估；最后把k次评估指标的平均值作为最终的评估指标。
 - **自助法**：对于总数为n的样本集合，进行n次有放回的随机抽样，得到大小为n的训练集。n次采样过程中，有的样本会被重复采样，有的样本没有被抽出过，将这些没有被抽出的样本作为验证集，进行模型验证。



<br>
### 2.5 超参数调优

 - **网格搜索**
 - **随机搜索**：随机搜索一般会比网格搜索要快一些，但是和网格搜索的快速版一样，它的结果也是没法保证的。
 - **贝叶斯优化算法**：贝叶斯优化算法则充分利用了之前的信息。贝叶斯优化算法通过对目标函数形状进行学习，找到使目标函数向全局最优值提升的参数。


<br>
### 2.6 过拟合与欠拟合

 - **过拟合**：获得更多的训练数据，降低模型复杂度，正则化方法，集成学习方法。
 - **欠拟合**：添加新特征，增加模型复杂度，减小正则化系数。






<br>
<br>

***

## 3 经典算法

### 3.1 支持向量机

 1. 对于任意线性可分的两组点，它们在SVM分类的超平面上的投影都是线性不可分的
 2. 若给定训练集中不存在两个点在同一位置，则存在一组参数{α1,...,αm,b}以及参数γ使得该SVM的训练误差为0
 3. 使用SMO算法训练的线性分类器并不一定能得到训练误差为0的模型，这是由于我们的优化目标改变了，并不再是使训练误差最小。
    


<br>
### 3.2 逻辑回归

&emsp;&emsp;一个事件的几率（odds）定义为该事件发生的概率与该事件不发生的概率的比值，那么逻辑回归可以看作是对于“y=1|x”这一事件的对数几率的线性回归。

**多项逻辑回归（Softmax Regression）**


<br>
### 3.3 决策树

 - C4.5——最大信息增益比
 - CART——最大基尼指数（Gini）


&emsp;&emsp;决策树的剪枝通常有两种方法，预剪枝（Pre-Pruning）和后剪枝（PostPruning）。预剪枝，即在生成决策树的过程中提前停止树的增长。而后剪枝，是在已生成的过拟合决策树上进行剪枝，得到简化版的剪枝决策树。


<br>
<br>

***

## 4 降维
### 4.1 PCA最大方差理论

&emsp;&emsp;此向量xi在ω（单位方向向量）上的投影坐标可以表示为xi·ω 。
&emsp;&emsp;x投影后的方差就是协方差矩阵的
特征值。我们要找到最大的方差也就是协方差矩阵最大的特征值，最佳投影方向
就是最大特征值所对应的特征向量。

 - （1）对样本数据进行中心化处理。
 - （2）求样本协方差矩阵。
 - （3）对协方差矩阵进行特征值分解，将特征值从大到小排列。
 - （4）取特征值前d大对应的特征向量ω1,ω2,...,ωd，通过以下映射将n维样本映射到d维


### 4.2 PCA最小平方误差
&emsp;&emsp;最小化的就是所有点到直线的距离平方之和。

![enter description here](http://img.zzhc321.xyz/blog/1657627144676.png)


### 4.3 线性判别分析
&emsp;&emsp;线性判别分析（Linear Discriminant  Analysis，LDA）是一种有监督学习算法，同时经常被用来对数据进行降维。相比于PCA，LDA可以作为一种有监督的降维算法。最大化类间距离和最小化类内距离。

<br>
<br>

***

## 5 非监督学习

### 5.1 K均值聚类
&emsp;&emsp;它的基本思想是，通过迭代方
式寻找K个簇（Cluster）的一种划分方案，使得聚类结果对应的代价函数最小。特
别地，代价函数可以定义为各个样本距离所属簇中心点的误差平方和

![enter description here](http://img.zzhc321.xyz/blog/1657628634826.png)

 - （1）数据预处理，如归一化、离群点处理等。
 - （2）随机选取K个簇中心，记为 。
 - （3）定义代价函数： 。
 - （4）令t=0,1,2,…为迭代步数，重复下面过程直到 J 收敛：


**调优:**

 - 数据归一化和离群点处理。
 - 合理选择K值。
 - 采用核函数。


**缺点：**

 - （1）需要人工预先确定初始K值，且该值和真实的数据分布未必吻合。
 - （2）K均值只能收敛到局部最优，效果受到初始值很大。
 - （3）易受到噪点的影响。
 - （4）样本点只能被划分到单一的类中。

**改进：**

&emsp;&emsp;假设已经选取了n个初始聚类中心（0<n<K），则在选取第n+1个聚类中心时，距离当前n个聚类中心越远的点会有更高的概率被选为第n+1个聚类中心。在选取第一个聚类中心（n=1）时同样通过随机的方法。


### 5.2 高斯混合模型

&emsp;&emsp;高斯混合模型假设每个簇的数据都是符合高斯分布（又叫正态分布）的，当前数据呈现的分布就是各个簇的高斯分布叠加在一起的结果。
![enter description here](http://img.zzhc321.xyz/blog/1657629049423.png)


### 5.3 自组织映射神经网络



<br>
<br>

***

## 6 概率图模型

&emsp;&emsp;用观测结点表示观测到的数据，用隐含结点表示潜在的知识，用边来描述知识与数据的相互关系，最后基于这样的关系图获得一个概率分布，非常“优雅”地解决了问题。




### 6.1 生成式模型与判别式模型


&emsp;&emsp;假设可观测到的变量集合为X，需要预测的变量集合为Y，其他的变量集合为Z。生成式模型是对联合概率分布P(X,Y,Z)进行建模，在给定观测集合X的条件下，通过计算边缘分布来得到对变量集合Y的推断。
![enter description here](http://img.zzhc321.xyz/blog/1657873686653.png)



判别式模型是直接对条件概率分布P(Y,Z|X)进行建模，然后消掉无关变量Z就可以得到对变量集合Y的预测，即
![enter description here](http://img.zzhc321.xyz/blog/1657873692463.png)






<br>
<br>

***

## 7 优化算法

<center>机器学习算法 = 模型表征 + 模型评估 + 优化算法。</center>



- 0-1损失
![enter description here](http://img.zzhc321.xyz/blog/1657874207119.png)



- 0-1损失的一个代理损失函数是Hinge损失函数

![enter description here](http://img.zzhc321.xyz/blog/1657874235563.png)


- Logistic损失函数也是0-1损失函数的凸上界

![enter description here](http://img.zzhc321.xyz/blog/1657874253510.png)


- 交叉熵损失

![enter description here](http://img.zzhc321.xyz/blog/1657874278384.png)


### 7.1 经典优化算法

经典的优化算法可以分为**直接法**和**迭代法**两大类。

随机梯度下降法用单个训练数据即可对模型参数进行一次更新，大大加快了收敛速率。该方法也非常适用于数据源源不断到来的在线更新场景。


### 7.2 梯度下降的改进

- 动量（Momentum）方法：当来到鞍点中心处，在惯性作用下继续前行，从而有机会冲出这片平坦的陷阱。
- AdaGrad方法：。AdaGrad方法采用“历史梯度平方和”来衡量不同参数的梯度的稀疏性，取值越小表明越稀疏。
- Adam方法：Adam方法将惯性保持和环境感知这两个优点集于一身。

### 7.3 L1正则化与稀疏性
&emsp;&emsp;稀疏性，说白了就是模型的很多参数是0。这相当于对模型进行了一次特征选择，只留下一些比较重要的特征，提高模型的泛化能力，降低过拟合的可能。

&emsp;&emsp;“带正则项”和“带约束条件”是等价（类似）的。如果原问题目标函数的最优解不是恰好落在解空间内，那么约束条件下的最优解一定是在解空间的边界上，而L1“棱角分明”的解空间显然更容易与目标函数等高线在角点碰撞，从而产生稀疏解。


<br>
<br>

***

## 8 采样


&emsp;&emsp;采样本质上是对随机现象的模拟，根据给定的概率分布，来模拟产生一个对的随机事件。采样可以让人们对随机事件及其产生过程有更直观的认识。例如，通过对二项分布的采样，可以模拟“抛硬币出现正面还是反面”这个随机事件，进而模拟产生一个多次抛硬币出现的结果序列，或者计算多次抛硬币后出现正面的频率。

&emsp;&emsp;采样得到的样本集也可以看作是一种非参数模型，即用较少量的
样本点（经验分布）来近似总体分布，并刻画总体分布中的不确定性












<br>
<br>

***

## 9 前向神经网络



&emsp;&emsp;由于从输入到输出的过程中不存在与模型自身的反馈连接，此类模型被称为“前馈”。

深度前馈网络通常由多个函数复合在一起来表示，该模型与一个有向无环图相关联，其中图则描述了函数的复合方式，例如“链式结构”f(x)=f(f(f()))链的全长定义为网络模型的“深度”。


### 9.1 常用激活函数

 - sigmoid在z很大或很小时都会趋近于0，造成梯度消失的现象。
 - Tanh导数在z很大或很小时都会趋近于0，同样会出现“梯度消失”。实际上，Tanh激活函数相当于Sigmoid的平移。
 - ReLU的非饱和性可以有效地解决梯度消失的问题，提供相对宽的激活边界；ReLU的单侧抑制提供了网络的稀疏表达能力。但是会导致超过一定比例的神经元不可逆死亡，进而参数梯度无法更新。


<br>
<br>

***

## 10 循环神经网络

















<br>
<br>

***

## 11 强化学习

















<br>
<br>

***

## 12 集成学习

















<br>
<br>

***

## 13 GAN













































<br>
<br>

***

## 14 卷积神经网络




























<br>
<br>

***

## 15 循环神经网络

















<br>
<br>

***

## 16 图神经网络













<br>
<br>

***

## 





















<br>
<br>

***

## 
















<br>
<br>

***

## 















<br>
<br>

***

## 










<br>
<br>

***

## 














<br>
<br>

***

## 










<br>
<br>

***

## 











<br>
<br>

***

## 











<br>
<br>

***

## 











<br>
<br>

***

## 












<br>
<br>

***

## 















<br>
<br>

***

## 












<br>
<br>

***

## 













<br>
<br>

***

## 















<br>
<br>

***

## 



















<br>
<br>

***

## 
















<br>
<br>

***

## 

















<br>
<br>

***

## 




















<br>
<br>

***

## 















<br>
<br>

***

## 

















